#!/bin/bash

printf "\n\n===== Axolotol Fine Tuning run\n"

export DATE=$(date +'%Y-%m-%d-%H-%M-%S')

printf "\n\n===== Vars:"
printf "\n=====   DATE: $DATE"
printf "\n=====   Model location: $MODEL_LOCATION_OR_NAME"
printf "\n=====   Tuning data: $TUNING_DATASET_LOCATION"
export NUM_GPUS=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
printf "\n\n===== GPUS: $NUM_GPUS"

if [[ -z "$HUGGINGFACE_TOKEN" ]]; then
  printf "\n\n===== Hugging Face Token: not found\n"
else
  printf "\n\n===== Hugging Face Token: found\n"
  huggingface-cli login --token $HUGGINGFACE_TOKEN
fi

export WANDB="\n"

if [[ -z "$WANDB_TOKEN" ]]; then
  export WANDB="
wandb_project: runnable-testing
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:
"
fi

cat > tune.yml <<- EOM

base_model: $MODEL_LOCATION_OR_NAME
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer

load_in_8bit: true
load_in_4bit: false
strict: false

datasets:
  - path: $TUNING_DATASET_LOCATION
    type:
      system_prompt: "\n"
      field_system: system
      field_instruction: input
      field_output: output
      format: "[INST] {instruction} [/INST]\n"
      no_input_format: "[INST] {instruction} [/INST]\n"

dataset_prepared_path:
val_set_size: 0.05
output_dir: /models/lora-out

sequence_len: 4096
sample_packing: false
pad_to_sequence_len: true

adapter: lora
lora_model_dir:
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:

wandb_project:
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 4
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true
s2_attention:

warmup_steps: 10
evals_per_epoch: 4
eval_table_size:
eval_sample_packing: False
eval_max_new_tokens: 128
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:

EOM

printf "\n\n===== Logging settings to bucket...\n"
mkdir /bucket/axolotol-out-$DATE
cp tune.yml /bucket/axolotol-out-$DATE/
printenv > /bucket/axolotol-out-$DATE/.env

printf "\n\n===== Preprocessing dataset...\n"
accelerate launch --multi_gpu --num_processes $NUM_GPUS -m axolotl.cli.preprocess tune.yml

printf "\n\n===== Running finetuning...\n"
# finetune lora
accelerate launch --multi_gpu --num_processes $NUM_GPUS -m axolotl.cli.train tune.yml

if [ ! -d /models/lora-out ]; then
  printf "\n\n===== Fine tuning failed, exiting.\n"
  exit 1
fi

printf "\n\n===== Copying model to bucket...\n"
cp -R /models/lora-out/ /bucket/axolotol-out-$DATE/

printf "\n\n==== Merging model...\n"
accelerate launch --multi_gpu --num_processes $NUM_GPUS -m axolotl.cli.merge_lora tune.yml --lora_model_dir="/models/lora-out\n"

printf "\n\n===== Copying merged model to bucket...\n"
cp -R /models/lora-out/merged /bucket/axolotol-out-$DATE/

# TODO: Do this in a build step first
printf "\n\n===== Installing ollama...\n"
apt-get update && apt-get install -y git python3 python3-pip
git clone https://github.com/ollama/ollama.git
cd ollama
git submodule init
git submodule update llm/llama.cpp
python3 -m venv llm/llama.cpp/.venv
source llm/llama.cpp/.venv/bin/activate
pip install -r llm/llama.cpp/requirements.txt
make -C llm/llama.cpp quantize

printf "\n\n==== Converting to ollama format...\n"
python llm/llama.cpp/convert.py /models/lora-out/merged --outtype f16 --outfile /models/ollama.bin

printf "\n\n==== Quantizing model...\n"
llm/llama.cpp/quantize /models/ollama.bin /models/ollama-quantized.bin q4_0

printf "\n\n===== Copying ollama formatted models to bucket...\n"
cp -R /models/ollama.bin /bucket/axolotol-out-$DATE/ollama.bin
cp -R /models/ollama-quantized.bin /bucket/axolotol-out-$DATE/ollama-quantized.bin

cat > /bucket/axolotol-out-$DATE/Modelfile <<- EOM
FROM ollama.bin
TEMPLATE "[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]"

EOM

printf "\n\n===== DONE!\n\n"
