#!/bin/bash

printf "\n\n===== Axolotol Fine Tuning run\n"

export DATE=$(date +'%Y-%m-%d-%H-%M-%S')

printf "\n\n===== Vars:"
printf "\n=====   DATE: $DATE"
printf "\n=====   Model location: $MODEL_LOCATION_OR_NAME"
printf "\n=====   Tuning data: $TUNING_DATASET_LOCATION"
export NUM_GPUS=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
printf "\n\n===== GPUS: $NUM_GPUS"

if [[ -z "$HUGGINGFACE_TOKEN" ]]; then
  printf "\n\n===== Hugging Face Token: not found\n"
else
  printf "\n\n===== Hugging Face Token: found\n"
  huggingface-cli login --token $HUGGINGFACE_TOKEN
fi

if [[ -z "$WANDB_TOKEN" ]]; then
  printf "\n\n===== WandB Token: not found\n"
  export WANDB="
wandb_mode: disabled
"
else
  printf "\n\n===== WandB Token: found\n"
  export WANDB="
wandb_project: $RELEASE_APP_NAME
wandb_entity:
wandb_watch:
wandb_name: axolotl-$DATE
wandb_log_model: $MODEL_LOCATION_OR_NAME
"
fi

cat > tune.yml <<- EOM

base_model: $MODEL_LOCATION_OR_NAME
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

load_in_8bit: false
load_in_4bit: true
strict: false

datasets:
  - path: $TUNING_DATASET_LOCATION
    type:
      system_prompt: "\n"
      field_system: system
      field_instruction: input
      field_output: output
      format: "[INST] {instruction} [/INST]\n"
      no_input_format: "[INST] {instruction} [/INST]\n"

dataset_prepared_path:
val_set_size: 0
output_dir: /models/lora-out

adapter: qlora
lora_model_dir:

sequence_len: 4096
sample_packing: true
pad_to_sequence_len: true

lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
lora_target_linear: true
lora_fan_in_fan_out:

wandb_project:
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 1
optimizer: paged_adamw_32bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true

warmup_steps: 10
evals_per_epoch: 4
eval_table_size:
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  pad_token: "<|end_of_text|>"

EOM

printf "\n\n===== Logging settings to bucket...\n"
mkdir /storage/bucket/axolotol-out-$DATE
cp tune.yml /storage/bucket/axolotol-out-$DATE/
printenv > /storage/bucket/axolotol-out-$DATE/.env

GPU_FLAGS=""
if (( $NUM_GPUS > 1 )); then
    GPU_FLAGS="--multi_gpu --num_processes ${NUM_GPUS}"
fi

printf "\n\n===== Preprocessing dataset...\n"
accelerate launch $GPU_FLAGS -m axolotl.cli.preprocess tune.yml

printf "\n\n===== Running finetuning...\n"
# finetune lora
accelerate launch $GPU_FLAGS -m axolotl.cli.train tune.yml

if [ ! -d /models/lora-out ]; then
  printf "\n\n===== Fine tuning failed, exiting.\n"
  exit 1
fi

printf "\n\n==== Merging model...\n"
accelerate launch $GPU_FLAGS -m axolotl.cli.merge_lora tune.yml --lora_model_dir="/models/lora-out"
