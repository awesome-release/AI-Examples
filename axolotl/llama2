#!/bin/bash

printf "\n\n===== Axolotol Fine Tuning run\n"

printf "\n\n===== Vars:"
printf "\n=====   Model location: $MODEL_LOCATION_OR_NAME"
printf "\n=====   Tuning data: $TUNING_DATASET_LOCATION"
export NUM_GPUS=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
printf "\n\n===== GPUS: $NUM_GPUS"

if [[ -z "$HUGGINGFACE_TOKEN" ]]; then
  printf "\n\n===== Hugging Face Token: not found\n"
else
  printf "\n\n===== Hugging Face Token: found\n"
  huggingface-cli login --token $HUGGINGFACE_TOKEN
fi

if [[ -z "$WANDB_TOKEN" ]]; then
  printf "\n\n===== WandB Token: not found\n"
  export WANDB="
wandb_mode: disabled
"
else
  printf "\n\n===== WandB Token: found\n"
  export WANDB="
wandb_project: $RELEASE_APP_NAME
wandb_entity:
wandb_watch:
wandb_name: axolotl-$DATE
wandb_log_model: $MODEL_LOCATION_OR_NAME
"
fi

cat > tune.yml <<- EOM

base_model: $MODEL_LOCATION_OR_NAME
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer

load_in_8bit: true
load_in_4bit: false
strict: false

datasets:
  - path: $TUNING_DATASET_LOCATION
    type:
      system_prompt: "\n"
      field_system: system
      field_instruction: input
      field_output: output
      format: "[INST] {instruction} [/INST]\n"
      no_input_format: "[INST] {instruction} [/INST]\n"

dataset_prepared_path:
val_set_size: 0.05
output_dir: /storage/lora-out

sequence_len: 4096
sample_packing: false
pad_to_sequence_len: true

adapter: lora
lora_model_dir:
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:

$WANDB

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 4
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true
s2_attention:

warmup_steps: 10
evals_per_epoch: 4
eval_table_size:
eval_sample_packing: False
eval_max_new_tokens: 128
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:

EOM

printf "\n\n===== Logging settings to bucket...\n"
cp tune.yml /storage/
printenv > /storage/.env

GPU_FLAGS=""
if (( $NUM_GPUS > 1 )); then
    GPU_FLAGS="--multi_gpu --num_processes ${NUM_GPUS}"
fi
echo $GPU_FLAGS

printf "\n\n===== Preprocessing dataset...\n"
accelerate launch $GPU_FLAGS -m axolotl.cli.preprocess tune.yml

printf "\n\n===== Running finetuning...\n"
# finetune lora
accelerate launch $GPU_FLAGS -m axolotl.cli.train tune.yml

if [ ! -d /storage/lora-out ]; then
  printf "\n\n===== Fine tuning failed, exiting.\n"
  exit 1
fi

printf "\n\n==== Merging model...\n"
accelerate launch $GPU_FLAGS -m axolotl.cli.merge_lora tune.yml --lora_model_dir="/storage/lora-out"

printf "\n\n===== Tuning done!"
