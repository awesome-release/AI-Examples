trainer:
  devices: 1
  num_nodes: 1
  accelerator: cpu
  logger: False # logger provided by exp_manager
  precision: bf16
  val_check_interval: 20
  max_steps: 50

tensor_model_parallel_size: 4
pipeline_model_parallel_size: 1
pipeline_model_parallel_split_rank: 0 # used for encoder and decoder model (0 for others)
gpt_model_file: "/tmp/tinyllama-1.1b-chat-v1.nemo" # GPT nemo file path
checkpoint_dir: null # checkpoint file dir. This is used to load the PTL checkpoint generated during the GPT training
checkpoint_name: null # PTL checkpoint file name, only used for PTL checkpoint loading
hparams_file: null # model configuration file, only used for PTL checkpoint loading
lora_model_path: "/tmp/nemo_experiments/megatron_gpt_peft_tuning/checkpoints/megatron_gpt_peft_tuning.nemo"
merged_model_path: "/tmp/tiny_merged_model.nemo"

model:
  micro_batch_size: 2
  global_batch_size: 8
