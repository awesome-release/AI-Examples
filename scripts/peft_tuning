#!/bin/bash

set -e

MODELS_ROOT_PATH=${MODELS_ROOT_PATH:-"/bucket/ai-models-tmp"}

# Setting default values for environment variables if not already set
export PEFT_TUNING_SCRIPT="${PEFT_TUNING_SCRIPT:-/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py}"
export CONCAT_SAMPLING_PROBS="${CONCAT_SAMPLING_PROBS:-[1]}"
export TP_SIZE="${TP_SIZE:-4}"
export PP_SIZE="${PP_SIZE:-1}"
export MODEL="${MODEL:-${MODELS_ROOT_PATH}/llama-2-7b-hf.nemo}"
export TRAIN_DS="${TRAIN_DS:-[${MODELS_ROOT_PATH}/pubmedqa/pubmedqa_train.jsonl]}"
export VALID_DS="${VALID_DS:-[${MODELS_ROOT_PATH}/pubmedqa/pubmedqa_val.jsonl]}"
export TEST_DS="${TEST_DS:-[${MODELS_ROOT_PATH}/pubmedqa/pubmedqa_test.jsonl]}"
export TEST_NAMES="${TEST_NAMES:-[pubmedqa]}"
export SCHEME="${SCHEME:-lora}"
export NVTE_FLASH_ATTN="${NVTE_FLASH_ATTN:-0}"
export NVTE_FUSED_ATTN="${NVTE_FUSED_ATTN:-0}"
export NPROC_PER_NODE="${NPROC_PER_NODE:-4}"
export GPT_MODEL_PATH="/workspace/model.nemo"

export CHECKPOINTS_PATH=/workspace/nemo_experiments/megatron_gpt_peft_tuning/checkpoints
export FINE_TUNED_CHECKPOINT_PATH=${FINE_TUNED_CHECKPOINT_PATH:-${MODELS_ROOT_PATH}/fine-tuned-checkpoints/fine_tuned_checkpoint.nemo}

echo "=========== Begin fine tuning with:"

echo "Model: $MODEL"
echo "Training dataset: $TRAIN_DS"
echo "Validation dataset: $VALID_DS"
echo "Test dataset: $TEST_DS"
echo "Test names: $TEST_NAMES"
echo "Scheme: $SCHEME"
echo "TP size: $TP_SIZE"
echo "PP size: $PP_SIZE"
echo "Concat sampling probabilities: $CONCAT_SAMPLING_PROBS"
echo "NVTE flash attention: $NVTE_FLASH_ATTN"
echo "NVTE fused attention: $NVTE_FUSED_ATTN"
echo "NPROC per node: $NPROC_PER_NODE"
echo "Fine tuned checkpoint output path: $FINE_TUNED_CHECKPOINT_PATH"

# If passed a folder rather than a .nemo file, convert it to nemo
if [ -d $MODEL ]; then
  export MODEL_FOLDER_NAME=$(basename $MODEL)
  export NEMO_MODEL_FILE_NAME="$MODEL_FOLDER_NAME.nemo"
  if [ -f $MODELS_ROOT_PATH/$NEMO_MODEL_FILE_NAME ]; then
    echo "===== .nemo format already exists, skip converting."
    echo "===== Copy .nemo from s3..."
    cp $MODELS_ROOT_PATH/$NEMO_MODEL_FILE_NAME /workspace/model.nemo
  else
    echo "===== Copying model: cp $MODELS_ROOT_PATH/$NEMO_MODEL_FOLDER_NAME /workspace/$MODEL_FOLDER_NAME..."
    cp $MODELS_ROOT_PATH/$MODEL_FOLDER_NAME /workspace/$MODEL_FOLDER_NAME --recursive
    echo "===== Converting HF model .nemo format..."
    python /opt/NeMo/scripts/nlp_language_modeling/convert_hf_llama_to_nemo.py --in-file=/workspace/$MODEL_FOLDER_NAME --out-file=/workspace/model.nemo
    echo "===== Converted HF model to .nemo!"
    # Store the .nemo file in s3
    echo "===== Storing /workspace/*.nemo model to $MODELS_ROOT_PATH/..."
    cp /workspace/model.nemo $MODELS_ROOT_PATH/$NEMO_MODEL_FILE_NAME
    echo "===== Done writing .nemo to s3"
  fi
fi

# If the .nemo file doesn't already exist from the conversion step, copy it from s3
if [ ! -f $GPT_MODEL_PATH ]; then
  cp $MODEL $GPT_MODEL_PATH
fi

# Default options
PEFT_OPTIONS="
  trainer.devices=4
  trainer.num_nodes=1
  trainer.precision=bf16-mixed
  trainer.val_check_interval=20
  trainer.max_steps=50
  model.megatron_amp_O2=False
  ++model.mcore_gpt=True
  model.tensor_model_parallel_size=${TP_SIZE}
  model.pipeline_model_parallel_size=${PP_SIZE}
  model.micro_batch_size=1
  model.global_batch_size=4
  model.restore_from_path=${GPT_MODEL_PATH}
  model.data.train_ds.num_workers=0
  model.data.validation_ds.num_workers=0
  model.data.train_ds.file_names="[${TRAIN_DS}]"
  model.data.train_ds.concat_sampling_probabilities=[1.0]
  model.data.validation_ds.file_names="[${VALID_DS}]"
  model.peft.peft_scheme=${SCHEME}
"

TORCH_OPTIONS="
  --nproc_per_node=${NPROC_PER_NODE}
"

echo "===== Begin fine tuning..."
torchrun \
  $TORCH_OPTIONS \
  $PEFT_TUNING_SCRIPT \
  $PEFT_OPTIONS

export CHECKPOINT_PATH="$CHECKPOINTS_PATH/model_peft_tuning_checkpoint.nemo"

if [ ! -f $CHECKPOINT_PATH ]; then
  echo "Error fine tuning: Checkpoint not found at $CHECKPOINT_PATH"
  exit 1
fi

echo "Copying fine-tuned checkpoint $CHECKPOINT_PATH to $RAW_OUTPUT_BUCKET_PATH"

mkdir -p $(dirname $FINE_TUNED_CHECKPOINT_PATH)
aws s3 cp $CHECKPOINT_PATH $RAW_OUTPUT_BUCKET_PATH
echo "===== Successfully copied fine-tuned checkpoint"

echo "===== Merging checkpoint into model"
export MERGED_MODEL_PATH="/workspace/merged.nemo"
cd /opt/NeMo
python scripts/nlp_language_modeling/merge_lora_weights/merge.py
echo "===== Successfully merge models"

aws s3 cp /workspace/merged.nemo /bucket/tuned.nemo $RAW_OUTPUT_BUCKET_PATH.tuned.nemo

echo "===== Exporting model to triton engine"
/bin/triton export \
  --nemo_checkpoint /workspace/merged.nemo \
  --model_repository /workspace/merged_triton_model

aws s3 cp /workspace/tuned_triton_model "${RAW_OUTPUT_BUCKET_PATH}triton_engine"