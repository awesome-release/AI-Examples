#!/bin/bash

set -e

MODELS_ROOT_PATH=${MODELS_ROOT_PATH:-"/bucket/ai-models-tmp"}

# Setting default values for environment variables if not already set
export PEFT_TUNING_SCRIPT="${PEFT_TUNING_SCRIPT:-/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py}"
export DATE=$(date +'%Y-%m-%d-%H-%M-%S')
export CONCAT_SAMPLING_PROBS="${CONCAT_SAMPLING_PROBS:-[1]}"
export TP_SIZE="${TP_SIZE:-4}"
export PP_SIZE="${PP_SIZE:-1}"
export MODEL="${MODEL:-${MODELS_ROOT_PATH}/llama-2-7b-hf.nemo}"
export TRAIN_DS="${TRAIN_DS:-[${MODELS_ROOT_PATH}/pubmedqa/pubmedqa_train.jsonl]}"
export VALID_DS="${VALID_DS:-[${MODELS_ROOT_PATH}/pubmedqa/pubmedqa_val.jsonl]}"
export TEST_DS="${TEST_DS:-[${MODELS_ROOT_PATH}/pubmedqa/pubmedqa_test.jsonl]}"
export TEST_NAMES="${TEST_NAMES:-[pubmedqa]}"
export SCHEME="${SCHEME:-lora}"
export NVTE_FLASH_ATTN="${NVTE_FLASH_ATTN:-0}"
export NVTE_FUSED_ATTN="${NVTE_FUSED_ATTN:-0}"
export NPROC_PER_NODE="${NPROC_PER_NODE:-4}"
export GPT_MODEL_PATH="/workspace/model.nemo"

export CHECKPOINTS_PATH=/workspace/nemo_experiments/megatron_gpt_peft_tuning/checkpoints
export FINE_TUNED_CHECKPOINT_PATH=${FINE_TUNED_CHECKPOINT_PATH:-${MODELS_ROOT_PATH}/fine-tuned-checkpoints/fine_tuned_checkpoint.nemo}

echo "=========== Begin fine tuning with:"

echo "Model: $MODEL"
echo "Training dataset: $TRAIN_DS"
echo "Validation dataset: $VALID_DS"
echo "Test dataset: $TEST_DS"
echo "Test names: $TEST_NAMES"
echo "Scheme: $SCHEME"
echo "TP size: $TP_SIZE"
echo "PP size: $PP_SIZE"
echo "Concat sampling probabilities: $CONCAT_SAMPLING_PROBS"
echo "NVTE flash attention: $NVTE_FLASH_ATTN"
echo "NVTE fused attention: $NVTE_FUSED_ATTN"
echo "NPROC per node: $NPROC_PER_NODE"
echo "Fine tuned checkpoint output path: $FINE_TUNED_CHECKPOINT_PATH"

# If not passed an s3 url, download the model
# if [ ... ]
#   echo "===== Downloading model from HF to S3..."
#   git lfs install
#   git clone https://huggingface.co/$MODEL_NAME $MODELS_ROOT
# fi

# If passed a folder rather than a .nemo file, convert it to nemo
if [ -d $MODEL ]; then
  export MODEL_FOLDER_NAME=$(basename $MODEL)
  export NEMO_MODEL_FILE_NAME="$MODEL_FOLDER_NAME.nemo"
  if [ -f $MODELS_ROOT_PATH/$NEMO_MODEL_FILE_NAME ]; then
    echo "===== .nemo format already exists, skip converting."
    echo "===== Copy .nemo from s3..."
    cp $MODELS_ROOT_PATH/$NEMO_MODEL_FILE_NAME /workspace/model.nemo
  else
    echo "===== Copying model: cp $MODELS_ROOT_PATH/$NEMO_MODEL_FOLDER_NAME /workspace/$MODEL_FOLDER_NAME..."
    cp $MODELS_ROOT_PATH/$MODEL_FOLDER_NAME /workspace/$MODEL_FOLDER_NAME --recursive
    echo "===== Converting HF model .nemo format..."
    python /opt/NeMo/scripts/nlp_language_modeling/convert_hf_llama_to_nemo.py --in-file=/workspace/$MODEL_FOLDER_NAME --out-file=/workspace/model.nemo
    echo "===== Converted HF model to .nemo!"
    # Store the .nemo file in s3
    echo "===== Storing /workspace/*.nemo model to $MODELS_ROOT_PATH/..."
    cp /workspace/model.nemo $MODELS_ROOT_PATH/$NEMO_MODEL_FILE_NAME
    echo "===== Done writing .nemo to s3"
  fi
fi

# If the .nemo file doesn't already exist from the conversion step, copy it from s3
if [ ! -f $GPT_MODEL_PATH ]; then
  cp $MODEL $GPT_MODEL_PATH
fi

# Default options
PEFT_OPTIONS="
  trainer.devices=4
  trainer.num_nodes=1
  trainer.precision=bf16-mixed
  trainer.val_check_interval=20
  trainer.max_steps=50
  model.megatron_amp_O2=False
  ++model.mcore_gpt=True
  model.tensor_model_parallel_size=${TP_SIZE}
  model.pipeline_model_parallel_size=${PP_SIZE}
  model.micro_batch_size=1
  model.global_batch_size=4
  model.restore_from_path=${GPT_MODEL_PATH}
  model.data.train_ds.num_workers=0
  model.data.validation_ds.num_workers=0
  model.data.train_ds.file_names="[${TRAIN_DS}]"
  model.data.train_ds.concat_sampling_probabilities=[1.0]
  model.data.validation_ds.file_names="[${VALID_DS}]"
  model.peft.peft_scheme=${SCHEME}
"

TORCH_OPTIONS="
  --nproc_per_node=${NPROC_PER_NODE}
"

echo "===== Begin fine tuning..."
torchrun \
  $TORCH_OPTIONS \
  $PEFT_TUNING_SCRIPT \
  $PEFT_OPTIONS

export CHECKPOINT_PATH="$CHECKPOINTS_PATH/megatron_gpt_peft_tuning.nemo"

if [ ! -f $CHECKPOINT_PATH ]; then
  echo "Error fine tuning: Checkpoint not found at $CHECKPOINT_PATH"
  exit 1
fi

echo "===== Copying fine-tuned checkpoint $CHECKPOINT_PATH to $RAW_OUTPUT_BUCKET_PATH..."

mkdir -p $(dirname $FINE_TUNED_CHECKPOINT_PATH)
aws s3 cp $CHECKPOINT_PATH $RAW_OUTPUT_BUCKET_PATH
echo "===== Successfully copied fine-tuned checkpoint"

echo "===== Merging checkpoint into model..."
cd /opt/NeMo

export MERGED_MODEL_PATH="/workspace/merged.nemo"
python scripts/nlp_language_modeling/merge_lora_weights/merge.py

echo "===== Successfully merged checkpoint back into original model"
echo "===== Copying merged nemo file to s3..."
aws s3 cp $MERGED_MODEL_PATH $RAW_OUTPUT_BUCKET_PATH.tuned.nemo

echo "===== Converting .nemo to HF format"
wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/checkpoint_converters/convert_llama_nemo_to_hf.py
python ./convert_llama_nemo_to_hf.py \
  --input_name_or_path /workspace/merged.nemo \
  --output_path $MODELS_ROOT_PATH/$MODEL_FOLDER_NAME-$DATE.bin \
  --hf_input_path $MODELS_ROOT_PATH/$MODEL_FOLDER_NAME \
  --hf_output_path $MODELS_ROOT_PATH/$MODEL_FOLDER_NAME-$DATE-hf \
  --input_tokenizer $MODELS_ROOT_PATH/$MODEL_FOLDER_NAME/tokenizer.json \
  --hf_output_tokenizer $MODELS_ROOT_PATH/$MODEL_FOLDER_NAME-$DATE-hf/tokenizer.json

echo "===== Done fine tuning!"
